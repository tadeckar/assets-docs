[
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/add_base_table_partions_prc/",
	"title": "add_base_table_partions_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Adds a partition to tables defined in the partition_tables table. (Called from Glue ETL Job.)   More Details    Get a cursor on partition_tables table where partitionTag and mgmtSystemType match input params that selects tableName. Get a count of wfIds for the given partitionTag. If the count is 0:  Insert all input params into the base_table_partition_info table Open a read loop over the cursor that, if partition on table doesn\u0026rsquo;t already exist, adds the partition to the table.     \nReferenced Tables  base_table_partition_info partition_automation_logs partition_tables  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/alerts/",
	"title": "Alerts",
	"tags": [],
	"description": "",
	"content": "Alerts are loaded by the AlertsParquetDataLoader.py.\nSchema Staging Tables  alert_fn_dcc alert_fn_dcn alert_fn_ib_data alert_fn_meraki alert_fn_telemetry alert_hweox_dcc alert_hweox_dcn alert_hweox_ib_data alert_hweox_meraki alert_hweox_telemetry alert_psirt_dcc alert_psirt_dcn alert_psirt_ib_data alert_psirt_meraki alert_psirt_telemetry alert_sweox_dcc alert_sweox_dcn alert_sweox_ib_data alert_sweox_meraki alert_sweox_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/alerts/",
	"title": "Alerts Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class AlertsFn{ +Long createdAt +Long fieldNoticeId +Long updatedAt +String cavbuid +String caveat +String cavid +String createdBy +String customerId +String distributionCode +String equipmentType +String fieldNoticeName +String hostname +String hwId +String id +String managementAddress +String neId +String productFamily +String productId +String serialNumber +String softwareType +String updatedBy +String vulnerabilityReason +String vulnerabilityStatus +String wfId } class AlertsHwEox{ +Long createdAt +Long updatedAt +String bulletinName +String cavbuid +String cavid +String createdBy +String customerId +String equipmentType +String hardwareEoxId +String hostname +String hwId +String id +String managementAddress +String neId +String productId +String updatedBy +String wfId } class AlertsPsirt{ +Long createdAt +Long psirtId +Long updatedAt +String cavbuid +String caveat +String cavid +String createdBy +String customerId +String cveId +String cvssBaseScore +String cxLevel +String equipmentType +String headlineName +String hostname +String hwId +String id +String managementAddress +String neId +String productFamily +String productId +String publicReleaseInd +String severity +String softwareType +String softwareVersion +String solutionIds +String updatedBy +String usecaseIds +String vulnerabilityReason +String vulnerabilityStatus +String wfId } class AlertsSwEox{ +Long createdAt +Long updatedAt +String bulletinHeadline +String cavbuid +String cavid +String createdBy +String customerId +String equipmentType +String hostname +String id +String managementAddress +String neId +String productId +String softwareEoxId +String softwareType +String softwareVersion +String updatedBy +String wfId }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/amp_log_msg_prc/",
	"title": "amp_log_msg_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Inserts a row into the amp_data_merge_logs table given customerId, wfId, function_name, and msg, along with the current timestamp.\nReferenced Tables  amp_data_merge_logs  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/amp_subscription_notification_prc/",
	"title": "amp_subscription_notification_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 30 seconds.\n What does it do? Checks for rows in asset_data_load_notification in SUBMITTED state where dataSource is CSDF_SUBSCRIPTION. Transitions the rows to INPROGRESS state and calls the sub_data_process_wrap_prc stored procedure. Afterwards, updates the state of the rows to SUCCESS.   More Details    Get a cursor over asset_data_load_notification where  processingStatus is SUBMITTED and dataSource is CSDF_SUBSCRIPTION None of the rows for same customerId are INPROGRESS   Get counts for  rows in SUBMITTED state rows in INPROGRESS state   Get the max parallel limit of amp uploads from the upd_or_async_prop_master table. If the SUBMITTED count is \u0026gt; 0 and INPROGRESS count is less than the max parallel limit, open a read loop that:  Counts the number of INPROGRESS rows for the given customerId If the count is 0 (none INPROGRESS), get the wfId from a SUBMITTED row. Update the status to INPROGRESS of SUBMITTED rows with the wfId from the previous step. Call the sub_data_process_wrap_prc stored procedure with input data from the row. Update the status to SUCCESS on the affected rows.     \nReferenced Tables  amp_data_merge_logs asset_data_load_notification upd_or_async_prop_master  Referenced Stored Procedures  amp_log_msg_prc sub_data_process_wrap_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/amp_upload_notification_prc/",
	"title": "amp_upload_notification_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 30 seconds.\n What does it do? Checks for rows in asset_data_load_notification in SUBMITTED state where dataSource is CSDF_AMP_TELE. Transitions the rows to INPROGRESS state and calls the iso_data_process_wrap_prc stored procedure. Afterwards, updates the state of the rows to SUCCESS.   More Details    Get a cursor over the asset_data_load_notification table where:  processingStatus is SUBMITTED and dataSource is CSDF_AMP_TELE and None of the rows for same customerId are INPROGRESS   Get a counts for  SUBMITTED notifications and INPROGRESS notifications   Get the max parallel limit of amp uploads from the upd_or_async_prop_master table. If the SUBMITTED count is \u0026gt; 0 and INPROGRESS count is less than the max parallel limit, open a read loop that:  Counts the number of INPROGRESS rows for the given customerId If the count is 0 (none INPROGRESS), get the wfId from a SUBMITTED row. Update the status to INPROGRESS of SUBMITTED rows with the wfId from the previous step. Call the iso_data_process_wrap_prc stored procedure with input data from the row. Update the status to SUCCESS on the affected rows.     \nReferenced Tables  asset_data_load_notification amp_data_merge_logs upd_or_async_prop_master  Referenced Stored Procedures  amp_log_msg_prc iso_data_process_wrap_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/athena_asset_notification_prc/",
	"title": "athena_asset_notification_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 10 seconds.\n What does it do? Checks for any rows of the asset_data_load_notification table that are in the SUBMITTED state. If there are any, it updates the state to INPROGRESS, then runs the athena_dcc_dcn_data_process_prc procedure. Afterwards, it changes the state of those rows again to SUCCESS.\n  More Details    Gets a cursor over the asset_data_load_notification table for rows where:  processingStatus is SUBMITTED and dataSource is ATHENA and the customerId has no other rows where processingStatus is INPROGRESS   Gets counts of:  AMP subscriptions where processingStatus is SUBMITTED AMP subscriptions where processingStatus is INPROGRESS   Gets the max parallel limit for INPROGRESS uploads from upd_or_async_prop_master table If SUBMITTED count is \u0026gt; 0 and the INPROGRESS count is less than the max parallel limit:  Start a read loop that:  First checks for INPROGRESS count, if count \u0026gt; 0 then log info message and continue Get the wfId from a row Update rows from SUBMITTED to INPROGRESS processingStatus If any rows were updated, call the athena_dcc_dcn_data_process_prc with customerId and wfId Finally, set the processingStatus of the updated rows to SUCCESS        Referenced Tables  asset_data_load_notification upd_or_async_prop_master  Referenced Stored Procedures  amp_log_msg_prc athena_dcc_dcn_data_process_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/athena_dcc_dcn_data_process_prc/",
	"title": "athena_dcc_dcn_data_process_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Removes all rows from the athena_subscription_stg table that have a stale wfId. Calls stored procedures to process the athena subscriptions. Then, syncs the athena subscriptions to the all_asset_view and all_asset_track_view tables.   More Details    Deletes stale records from the athena_subscription_stg table where  customerId matches the stored procedure input param wfId does not match the stored procedure input param   Count the rows in the customer_partition_info table that match the customerId where partitionStatus is A. If the count is 1:  Get the wfId from the customer_partition_info table Call the following stored procedures with customerId/wfId as parameters:  athena_dcc_stg_data_process_prc athena_dcn_stg_data_process_prc athena_ib_stg_data_process_prc     Get a count of any rows in athena_subscription_stg where assetCategory is Cisco Plus that are not present in the all_asset_view table. If the count is \u0026gt; 0:  Update rows in all_asset_view so that fields match rows in athena_subscription_stg. Update rows in all_asset_track_view so that fields match rows in athena_subscription_stg.   Get a count of any rows in all_asset_view where assetCategory is Cisco Plus that are not present in the athena_subscription_stg table. If the count is \u0026gt; 0 update assetCategory to Cisco for the rows in all_asset_view and all_asset_track_view that matched previous count.   \nReferenced Tables  amp_data_merge_logs athena_subscription_stg customer_partition_info all_asset_view all_asset_track_view  Referenced Stored Procedures  amp_log_msg_prc athena_dcc_stg_data_process_prc athena_dcn_stg_data_process_prc athena_ib_stg_data_process_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/bulletins/",
	"title": "Bulletins",
	"tags": [],
	"description": "",
	"content": "Bulletins are loaded by the BulletinMasterParquetDataLoader.py.\nSchema Staging Tables  alert_pas_hw_eox_bulletin_dcc alert_pas_hw_eox_bulletin_dcn alert_pas_hw_eox_bulletin_ib_data alert_pas_hw_eox_bulletin_meraki alert_pas_hw_eox_bulletin_telemetry alert_pas_sw_eox_bulletin_dcc alert_pas_sw_eox_bulletin_dcn alert_pas_sw_eox_bulletin_ib_data alert_pas_sw_eox_bulletin_meraki alert_pas_sw_eox_bulletin_telemetry fn_bulletin_master_dcc fn_bulletin_master_dcn fn_bulletin_master_ib_data fn_bulletin_master_meraki fn_bulletin_master_telemetry sa_bulletin_master_dcc sa_bulletin_master_dcn sa_bulletin_master_ib_data sa_bulletin_master_meraki sa_bulletin_master_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/bulletins/",
	"title": "Bulletins Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class FnBulletin{ +Long fieldNoticeId +Long firstPublishDate +Long lastRevisionDate +String background +String caveat +String distributionCode +String fieldNoticeName +String fieldNoticeTypeCode +String fieldNoticeUrl +String generatedAt +String hardwareLevels +String isSerialNumberAvailableCode +String problemDescription +String problemSymptoms +String publish_user_id +String recordType +String recordVersion +String status +String upgradeProgam +String workaround } class PasHwEoxBulletin{ +Date createdDate +Long currentMilestone +Long currentMilestoneDate +Long endOfBuEngineeringSupportTacDate +Long endOfHardwareNewServiceAttachmentDate +Long endOfHardwareRoutineFailureAnalysisDate +Long endOfHardwareServiceContractRenewalDate +Long endOfLastDateOfSupport +Long endOfLastHardwareShipDate +Long endOfLifeExternalAnnouncementDate +Long endOfLifeInternalAnnouncementDate +Long endOfSaleDate +Long endOfSignatureReleasesDate +Long endOfSoftwareAvailabilityDate +Long endOfSoftwareLicenseAvailabilityDate +Long endOfSoftwareMaintenanceReleasesDate +Long endOfSoftwareVulnerabilityOrSecuritySupportDate +Long hardwareEoXId +Long nextMilestone +Long nextMilestoneDate +Long postDate +String bulletinName +String bulletinNumber +String bulletinPid +String bulletinURL +String generatedAt +String migrationInfo +String milestoneInfo +String recordType +String recordVersion } class PasSwEoxBulletin{ +Date createdDate +Long currentMilestone +Long currentMilestoneDate +Long endOfBuEngineeringSupportTacDate +Long endOfEngineeringDate +Long endOfHardwareNewServiceAttachmentDate +Long endOfHardwareRoutineFailureAnalysisDate +Long endOfHardwareServiceContractRenewalDate +Long endOfLastDateOfSupport +Long endOfLastHardwareShipDate +Long endOfLifeDate +Long endOfLifeExternalAnnouncementDate +Long endOfLifeInternalAnnouncementDate +Long endOfSaleDate +Long endOfSignatureReleasesDate +Long endOfSoftwareAvailabilityDate +Long endOfSoftwareLicenseAvailabilityDate +Long endOfSoftwareMaintenanceReleasesDate +Long endOfSoftwareVulnerabilityOrSecuritySupportDate +Long nextMilestone +Long nextMilestoneDate +Long postDate +Long softwareEoXId +String bulletinHeadline +String bulletinNumber +String bulletinURL +String generatedAt +String migrationInfo +String milestoneInfo +String recordType +String recordVersion } class SaBulletin{ +Long createdDate +Long documentId +Long internalAnnounceDate +Long psirtId +Long publishDate +Long revisedDate +Long signatureReleaseDate +Long softwareReleaseDate +Long softwareVossDate +String advisoryId +String affectedImageName +String alertStatusCd +String announceTypeName +String caveat +String codeName +String cveId +String cvssBaseScore +String cvssTemporalScore +String defectId +String detailText +String generatedAt +String headlineName +String mappingStateName +String psirtUrlText +String publicReleaseInd +String recordType +String recordVersion +String revisionNum +String severity +String summaryText }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/clean_invalid_status_prc/",
	"title": "clean_invalid_status_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs once every day.\n What does it do? Checks rows in base_table_partition_info table where lastUpdateDate is over 4 days ago, drops those partitions, and deletes the rows.   More Details    Get a cursor over base_table_partition_info where lastUpdateDate is over 4 days ago. Loop over the cursor, performing the following tasks  Call the drop_partions_prc stored procedure Delete rows from base_table_partition_info where  lastUpdateDate is over 2 days ago partitionStatus is I       \nReferenced Tables  base_table_partition_info partition_automation_logs  Referenced Stored Procedures  drop_partions_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/clean_invalid_wfid_status_sum_tables_prc/",
	"title": "clean_invalid_wfId_status_sum_tables_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs once every day.\n What does it do? Drops partitions for rows in customer_partition_info where partitionStatus is I and lastUpdateDate is greater than 4 days ago. Then, deletes the rows from customer_partition_info.   More Details    Get a cursor over customer_partition_info where:  partitionStatus is I and lastUpdateDate is greater than 4 days ago.   Open a read loop over the cursor that:  Calls the drop_partions_prc stored procedure with data from row as params. Deletes the row from customer_partition_info where lastUpdateDate is greater than 2 days ago.     \nReferenced Tables  customer_partition_info partition_automation_logs  Referenced Stored Procedures  drop_partions_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/clean_stale_datamerge_prc/",
	"title": "clean_stale_datamerge_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 30 minutes.\n What does it do? Updates status of long-running (over 2-day old) notifications from INPROGRESS to FAILED.   More Details    Gets a cursor over asset_inventory_notification where  processingStatus is INPROGRESS processingStart was over 2 days ago   Loop over cursor, updating each row\u0026rsquo;s processingStatus to FAILED   \nReferenced Tables  asset_inventory_notification data_merge_logs  Referenced Stored Procedures  log_msg_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/cleanup_longrunning_iso_data_prc/",
	"title": "cleanup_longrunning_iso_data_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 4 hours.\n What does it do? Checks for rows of asset_data_load_notification that have been INPROGRESS for over 2 hours. Updates the status of those rows to SUBMITTED. Also, updates rows that have been SCHEDULED for over 2 hours to RECEIVED.   More Details    Get a cursor over the asset_data_load_notification where  processingStart is over 2 hours ago, also not null/undefined, and processingStatus is INPROGRESS   Get a count of rows in the asset_data_load_notification table where  processingStart is over 2 hours ago, also not null/undefined, and processingStatus is INPROGRESS or SCHEDULED   If the count is \u0026gt; 0, open a read loop on the cursor that  Update the processingStatus to SUBMITTED and the processingEnd to the current time for any INPROGRESS rows that were started over 2 hours ago. Update the processingStatus to RECEIVED and the processingEnd to the current time for any SCHEDULED rows that were started over 2 hours ago.     \nReferenced Tables  asset_data_load_notification data_merge_logs  Referenced Stored Procedures  amp_log_msg_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/cli/",
	"title": "CLI",
	"tags": [],
	"description": "",
	"content": "CLI data is loaded by the CLIParquetDataLoader.py.\nSchema Staging Tables  cli_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/cli/",
	"title": "CLI Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class Cli { +String commandName +String commandOutput }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/config/",
	"title": "Config",
	"tags": [],
	"description": "",
	"content": "Config data is loaded by the ConfigParquetDataLoader.py.\nSchema Staging Tables  config_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/config/",
	"title": "Config Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class Config{ +String config +String configName }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/contracts/",
	"title": "Contracts",
	"tags": [],
	"description": "",
	"content": "Contracts are loaded by the ContractParquetDataLoader.py.\nSchema Staging Tables  contractcoverage_dcc contractcoverage_dcn contractcoverage_ib_data contractcoverage_meraki contractcoverage_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/contracts/",
	"title": "Contracts Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class ContractNE{ +Boolean isCollector +Long InstallationDate +Long contractEndDate +Long contractId +Long contractStartDate +Long coverageEndDate +Long coverageStartDate +Long generatedAt +Long instanceId +Long lastCoverageEndDate +Long warrantyEndDate +Long warrantyStartDate +String BillToCrPartyId +String CavBUName +String CavName +String ComponentType +String InstallAtCustomerName +String InstallAtSiteGuId +String InstallAtSiteGuName +String InstallAtSitePartyId +String InstallCustomerAddrLine3 +String InstallationQuantity +String InstanceCreationDtm +String InstanceLastUpdateDtm +String PartnerBeName +String ProductShipmentConfirmedDtm +String SalesOrderNumber +String ServiceLevel +String billToAddress1 +String billToAddress2 +String billToCity +String billToCountry +String billToPostalCode +String billToProvince +String billToSiteName +String billToState +String cavbuid +String cavid +String collectorId +String contractNumber +String contractOwner +String contractStatus +String coverageStatus +String createdAt +String createdBy +String customerId +String customerName +String cxLevel +String endOfSaleDate +String genPartyId +String hwId +String ibAvailabilityFlag +String id +String installAddress1 +String installAddress2 +String installCity +String installCountry +String installPostalCode +String installProvince +String installSiteName +String installState +String instanceStatus +String itemTypeCode +String lastDateOfSupport +String licenseTermInMonth +String managedNeId +String mgmtSystemAddr +String mgmtSystemHostname +String mgmtSystemId +String mgmtSystemType +String neId +String parentInstanceId +String parentSerialNumber +String partnerBeGeoId +String partnerBeGeoName +String partnerBeId +String prdtSetupClassificationCd +String productEndCrPartyIdInt +String productId +String serialNumber +String serviceLineStatus +String serviceProgram +String sfcFlag +String slaCode +String slaDescription +String sourceNeId +String telemetryAvailablityFlag +String termsAndContentCd +String updatedAt +String updatedBy +String warrantyType +String wfId }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/services/cp-asset-group-api/",
	"title": "cp-asset-group-api",
	"tags": [],
	"description": "",
	"content": " View Source  Invokes Stored Procedures  contract_group_device_sum_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/types/data-load-types/",
	"title": "Data Load Types",
	"tags": [],
	"description": "",
	"content": " View Source  CIBES BULLETIN_MSTR_PARQ_DATA_LOAD = \u0026#39;CIBES-BULLETIN-MASTER\u0026#39; NETWORK_ELEMENTS_PARQ_DATA_LOAD = \u0026#39;CIBES-NETWORK-ELEMENTS\u0026#39; EQUIPMENTS_PARQ_DATA_LOAD = \u0026#39;CIBES-EQUIPMENTS\u0026#39; ALERTS_PARQ_DATA_LOAD = \u0026#39;CIBES-ALERTS\u0026#39; CONTRACTS_PARQ_DATA_LOAD = \u0026#39;CIBES-CONTRACTS\u0026#39; DNAC DNAC_BULLETIN_MSTR_PARQ_DATA_LOAD = \u0026#39;DNAC-BULLETIN-MASTER\u0026#39; DNAC_NETWORK_ELEMENTS_PARQ_DATA_LOAD = \u0026#39;DNAC-NETWORK-ELEMENTS\u0026#39; DNAC_EQUIPMENTS_PARQ_DATA_LOAD = \u0026#39;DNAC-EQUIPMENTS\u0026#39; DNAC_ALERTS_PARQ_DATA_LOAD = \u0026#39;DNAC-ALERTS\u0026#39; DNAC_CONTRACTS_PARQ_DATA_LOAD = \u0026#39;DNAC-CONTRACTS\u0026#39; DNAC_CLI_PARQ_DATA_LOAD = \u0026#39;DNAC-CLI\u0026#39; DNAC_CONFIG_PARQ_DATA_LOAD = \u0026#39;DNAC-CONFIG\u0026#39; Hybrid HYBRID_SUBS_DATA_LOAD = \u0026#34;ATHENA-SUBSCRIPTION\u0026#34; IBES BULLETIN_MSTR_DATA_LOAD = \u0026#39;IBES-BULLETIN-MASTER\u0026#39; NETWORK_ELEMENTS_DATA_LOAD = \u0026#39;IBES-NETWORK-ELEMENTS\u0026#39; EQUIPMENTS_DATA_LOAD = \u0026#39;IBES-EQUIPMENTS\u0026#39; ALERTS_DATA_LOAD = \u0026#39;IBES-ALERTS\u0026#39; CONTRACTS_DATA_LOAD = \u0026#39;IBES-CONTRACTS\u0026#39; FEATURES_DATA_LOAD = \u0026#39;IBES-FEATURES\u0026#39; License LICENSE_MERAKI = \u0026#39;LICENSE-MERAKI\u0026#39; XAAS NETWORK_ELEMENTS_SUBS_PARQ_DATA_LOAD = \u0026#39;XAAS-NETWORK-ELEMENTS-SUBS\u0026#39; SUBSCRIPTION_PARQ_DATA_LOAD = \u0026#34;XAAS-SUBSCRIPTION\u0026#34; Other CONTRACT_SUMMARY_DATA_LOAD = \u0026#39;CCCS-CONTRACT-SUMMARY\u0026#39; "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/",
	"title": "Database",
	"tags": [],
	"description": "",
	"content": "  Recurring Events   Staging Tables   Stored Procedures   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/data-file-manager/",
	"title": "DataFileManager",
	"tags": [],
	"description": "",
	"content": " View Source  The DataFileManager is used for all data loading tasks. Its purpose is to choose one of the following Managers based on a given File Type argument:\nATHENA:\nHybridSubscriptionDataLoadManager\nINVENTORY_DATA_RECEIVED:\nInventoryDataLoadManager\nMERAKI_LICENSE:\nLicenseDataLoadManager\nCIBES_INVENTORY_DATA_RECEIVED and DNAC_DATA_RECEIVED and all other file types:\nParquetInvDataLoadManager\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/data-load-manager/",
	"title": "DataLoadManager",
	"tags": [],
	"description": "",
	"content": " View Source  Similar to DataFileManager, the DataLoadManager simply picks a Loader to use given a Data Load Type.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/delete_raw_data_wfid_prc/",
	"title": "delete_raw_data_wfid_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Deletes rows matching customerId/wfId from tables based on a given mgmtSystemType.   More Details    Deletes rows that match customerId/wfId from base_table_partition_info. Deletes rows that match customerId/wfId from tables based on mgmtSystemType.  When mgmtSystemType is DNAC, deletes from tables:  active_dnac_ne alert_fn_telemetry alert_hweox_telemetry alert_pas_hw_eox_bulletin_dnac alert_pas_sw_eox_bulletin_dnac alert_psirt_telemetry alert_sweox_telemetry asset_feature_telemetry equipment_telemetry fn_bulletin_master_dnac networkelement_telemetry sa_bulletin_master_dnac   When mgmtSystemType is APIC, deletes from tables:  alert_fn_dcn alert_hweox_dcn alert_pas_hw_eox_bulletin_dcn alert_pas_sw_eox_bulletin_dcn alert_psirt_dcn alert_sweox_dcn equipment_dcn fn_bulletin_master_dcn networkelement_dcn sa_bulletin_master_dcn   When mgmtSystemType is CSDFIB, deletes from tables:  alert_fn_ib_data alert_hweox_ib_data alert_pas_hw_eox_bulletin_ib_data alert_pas_sw_eox_bulletin_ib_data alert_psirt_ib_data alert_sweox_ib_data contractcoverage_ib_data equipment_ib_data fn_bulletin_master_ib_data networkelement_ib_data sa_bulletin_master_ib_data   When mgmtSystemType is MERAKI, deletes from tables:  alert_fn_meraki alert_hweox_meraki alert_pas_hw_eox_bulletin_meraki alert_pas_sw_eox_bulletin_meraki alert_psirt_meraki alert_sweox_meraki contractcoverage_meraki equipment_meraki fn_bulletin_master_meraki networkelement_meraki sa_bulletin_master_meraki   When mgmtSystemType is DCC, deletes from tables:  alert_fn_dcc alert_hweox_dcc alert_pas_hw_eox_bulletin_dcc alert_pas_sw_eox_bulletin_dcc alert_psirt_dcc alert_sweox_dcc equipment_dcc fn_bulletin_master_dcc networkelement_dcc sa_bulletin_master_dcc       \nReferenced Tables  active_dnac_ne alert_fn_dcc alert_fn_dcn alert_fn_ib_data alert_fn_meraki alert_fn_telemetry alert_hweox_dcc alert_hweox_dcn alert_hweox_ib_data alert_hweox_meraki alert_hweox_telemetry alert_pas_hw_eox_bulletin_dcc alert_pas_hw_eox_bulletin_dcn alert_pas_hw_eox_bulletin_dnac alert_pas_hw_eox_bulletin_ib_data alert_pas_hw_eox_bulletin_meraki alert_pas_sw_eox_bulletin_dcc alert_pas_sw_eox_bulletin_dcn alert_pas_sw_eox_bulletin_dnac alert_pas_sw_eox_bulletin_ib_data alert_pas_sw_eox_bulletin_meraki alert_psirt_dcc alert_psirt_dcn alert_psirt_ib_data alert_psirt_meraki alert_psirt_telemetry alert_sweox_dcc alert_sweox_dcn alert_sweox_ib_data alert_sweox_meraki alert_sweox_telemetry asset_feature_telemetry base_table_partition_info contractcoverage_ib_data contractcoverage_meraki equipment_dcc equipment_dcn equipment_ib_data equipment_meraki equipment_telemetry fn_bulletin_master_dcc fn_bulletin_master_dcn fn_bulletin_master_dnac fn_bulletin_master_ib_data fn_bulletin_master_meraki networkelement_dcc networkelement_dcn networkelement_ib_data networkelement_meraki networkelement_telemetry sa_bulletin_master_dcc sa_bulletin_master_dcn sa_bulletin_master_dnac sa_bulletin_master_ib_data sa_bulletin_master_meraki  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/drop_partions_prc/",
	"title": "drop_partions_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Drops a partition from all tables that it is a part of.   More Details    Gets a cursor over partition_tables where partitionTag matches the given parameter. Create a read loop on the cursor that:  Gets a count of rows in INFORMATION_SCHEMA db PARTITIONS table where:  TABLE_NAME matches the rows tableName and partition_description matches the given partitionValue parameter.   If the count from the previous step is \u0026gt; 1:  Get the rows from the INFORMATION_SCHEMA db PARTITIONS table that match the row from the cursor Use the partition_name field from the result to drop that partition from each table       \nReferenced Tables  partition_automation_logs partition_tables  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/drop_pending_wfid_status_sum_tables_prc/",
	"title": "drop_pending_wfid_status_sum_tables_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs once every day.\n What does it do? Calls the drop_partions_prc stored procedure on rows of the customer_partition_info table that have been in P state for over a day.   More Details    Get a cursor over customer_partition_info where  lastUpdateDate is \u0026gt; 1 day go and partitionStatus is P   Open a read loop on the cursor that  Calls the drop_partions_prc stored procedure with fields from the row Deletes the row from customer_partition_info     \nReferenced Tables  customer_partition_info partition_automation_logs  Referenced Stored Procedures  drop_partions_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/drop_stale_partions_prc/",
	"title": "drop_stale_partions_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs once every day.\n What does it do? Drops partitions in which the partition_description in the INFORMATION_SCHEMA db PARTITIONS table does not match up to any partitionValues in the base_table_partition_info or customer_partition_info tables.   More Details    Gets a cursor over INFORMATION_SCHEMA db PARTITIONS table where:  partition_description is not in any partitionValues from base_table_partition_info and partition_description is not in any partitionValues from customer_partition_info   Opens a read loop on the cursor that drops each partition from the table it\u0026rsquo;s part of. (Note: Read loop skips row where partition_id is pp1.)   \nReferenced Tables  base_table_partition_info customer_partition_info partition_automation_logs  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/equipments/",
	"title": "Equipments",
	"tags": [],
	"description": "",
	"content": "Equipments are loaded by the EQParquetDataLoader.py.\nSchemas Staging Tables  equipment_dcc equipment_dcn equipment_ib_data equipment_meraki equipment_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/equipments/",
	"title": "Equipments Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class Equipment{ +Boolean isCollector +Long createdAt +Long generatedAt +Long updatedAt +String cavbuid +String cavid +String collectedPid +String collectedSerialNum +String collectorId +String containingHwId +String createdBy +String customerId +String equipmentType +String genPartyId +String hostname +String hwId +String ibAvailabilityFlag +String id +String installProductType +String itemTypeCode +String managedNeId +String managementAddress +String manufacturer +String mgmtSystemAddr +String mgmtSystemHostname +String mgmtSystemId +String mgmtSystemType +String neId +String pceMultiPid +String pcePhysicalType +String pcePid +String pceProductDescription +String pceProductFamily +String pceProductName +String pceProductType +String pceRuleId +String prdtSetupClassificationCd +String productClass +String productDescription +String productFamily +String productId +String productName +String productType +String productsubtype +String serialNumber +String snasItemType +String snasProductFamily +String snasSerialNumber +String snasValidationCode +String snasValidationSource +String sourceNeId +String tags +String telemetryAvailablityFlag +String termsAndContentCd +String updatedBy +String wfId }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/types/file-types/",
	"title": "File Types",
	"tags": [],
	"description": "",
	"content": " View Source  IBES_INV_UPLOAD = \u0026#39;INVENTORY_DATA_RECEIVED\u0026#39; CIBES_INV_UPLOAD = \u0026#39;CIBES_INVENTORY_DATA_RECEIVED\u0026#39; MERAKI_LICENSE = \u0026#39;MERAKI_LICENSE\u0026#39; CC_CONTACT_SUM_UPLOAD = \u0026#39;CC-CONTACT-SUM-UPLOAD\u0026#39; CIBES_XAAS_INVENTORY_UPLOAD = \u0026#39;XAAS_INVENTORY_DATA_RECEIVED\u0026#39; HYBRID_CLOUD =\u0026#39;ATHENA\u0026#39; DNAC_UPLOAD = \u0026#39;DNAC_DATA_RECEIVED\u0026#39; "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "Table of Contents  Database   Ingestion   Services   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/hybrid-subscriptions/",
	"title": "Hybrid Subscriptions",
	"tags": [],
	"description": "",
	"content": "Hybrid Subscriptions are loaded by the HybridSubscriptionParquetDataLoader.py.\nSchema Staging Tables  athena_subscription_stg  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/hybrid-subscriptions/",
	"title": "Hybrid Subscriptions Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class Subscription{ +Integer erpSalesOrderNumber +Integer licenseTermInMonths +Long subscriptionCreateDate +Long subscriptionProductQuantity +Long termEndDate +Long termStartDate +String contractEntitlementDescr +String contractNumber +String coverageStatus +String endCustomerGuAddressLine1 +String endCustomerGuAddressLine2 +String endCustomerGuAddressLine3 +String endCustomerGuAddressLine4 +String endCustomerGuCityName +String endCustomerGuCountry +String endCustomerGuPostalCd +String endCustomerGuState +String managedNeId +String monetizationTypeCd +String neId +String partnerBeGeoId +String partnerBeGeoName +String partnerBeId +String partnerBeName +String serviceLevel +String serviceLevelDescription +String serviceProgram +String subscriptionBillToSiteUseCustName +String subscriptionBillToSiteUseId +String subscriptionProductClass +String subscriptionReferenceId +String subscriptionStatus +String subscriptionType +String termsAndContentCd +String webOrderId } class HybridSubscription{ +Long deploymentEndDate +Long deploymentStartDate +Long subscriptionEndDate +Long subscriptionStartDate +String assetCategory +String cavBuId +String cavBuName +String cavId +String cavName +String deploymentId +String offerType +String portalCustomerId +String primaryDataRegion +String serialNumber +String subscriptionRefId +String technology +String usageStatus +String useCaseOrworkLoad }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/hybrid-subscription-data-load-manager/",
	"title": "HybridSubscriptionDataLoadManager",
	"tags": [],
	"description": "",
	"content": " View Source  Loads data of the HYBRID_CLOUD File Type.\nFirstly, the HSDLM checks whether or not data for the given wfId has already been loaded. If the data has already been loaded, the data is cleared from the athena_subscription_stg staging table for the given customerId/wfId before execution proceeds to the next step.\nNext, a thread is spun up to load the data, executing the DataLoadManager, which in turn executes a Loader for the Hybrid Data Load Type.\nThe NotificationManager is also used record the data load processing start/end times, as well as to delete rows when wfId data has already been loaded.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/",
	"title": "Ingestion",
	"tags": [],
	"description": "",
	"content": "The repositories associated with AWS Glue are:\n cp-asset-data-pipeline: Ingest Data to MySQL from Parquet files in S3 cp-asset-data-export-pipeline: Export data to consumers (Insights)  Overview At a high level, a Lambda function listens for SQS events. Upon receiving, the Glue ETL Job is triggered. The job downloads files from an S3 bucket, parses them, and stores results in staging tables of the RDS MySQL database.\nThe SQS message\u0026rsquo;s recordType property signals the type of data that should be loaded. The graph below depicts the flow of logic through the Glue ETL Job.\ngraph TD; RT{SQS recordType} --|CSDF_AMP_TELE| PINV(ParqeutInvDataLoadManager) RT --|CSDF_SUBSCRIPTION| PINV RT --|CIBES_INVENTORY_DATA_RECEIVED| PINV RT --|DNAC_DATA_RECEIVED| PINV RT --|MERAKI_LICENSE| LIC(LicenseDataLoadManager) RT --|XAAS_INVENTORY_DATA_RECEIVED| PINV RT --|DCC_SUB| PINV RT --|ATHENA| HYB(HybridSubscriptionDataLoadManager) PINV -- PINVD{SQS mgmtSystemType} PINVD --|DCC_SUB| DCC_SUBL(DataLoadManager) DCC_SUBL -- DCC_SUBL_NEPDL(NEParquetDataLoader) DCC_SUBL -- DCC_SUBL_SPDL(SubscriptionParquetDataLoader) PINVD --|DNAC_DL| DNAC_DLL(DataLoadManager) DNAC_DLL -- DNAC_DLL_NEPDL(NEParquetDataLoader) DNAC_DLL -- DNAC_DLL_EQPDL(EQParquetDataLoader) DNAC_DLL -- DNAC_DLL_APDL(AlertsParquetDataLoader) DNAC_DLL -- DNAC_DLL_BMPDL(BulletinMasterParquetDataLoader) DNAC_DLL -- DNAC_DLL_CPDL(ContractParquetDataLoader) DNAC_DLL -- DNAC_DLL_CLIPDL(CLIParquetDataLoader) DNAC_DLL -- DNAC_DLL_CONFIGPDL(ConfigParquetDataLoader) PINVD --|else| PINVD_ELSE(DataLoadManager) PINVD_ELSE -- ELSE_BMPDL(BulletinMasterParquetDataLoader) PINVD_ELSE -- ELSE_NEPDL(NEParquetDataLoader) PINVD_ELSE -- ELSE_EQPDL(EQParquetDataLoader) PINVD_ELSE -- ELSE_APDL(AlertsParquetDataLoader) PINVD_ELSE -- ELSE_CPDL(ContractParquetDataLoader) click PINV \"/pages/tadeckar/assets-docs/ingestion/managers/parquet-inv-data-load-manager/\" click DCC_SUBL \"/pages/tadeckar/assets-docs/ingestion/managers/data-load-manager/\" click DNAC_DLL \"/pages/tadeckar/assets-docs/ingestion/managers/data-load-manager/\" click PINVD_ELSE \"/pages/tadeckar/assets-docs/ingestion/managers/data-load-manager/\" click DCC_SUBL_NEPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/network-elements/\" click DCC_SUBL_SPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/subscriptions/\" click DNAC_DLL_NEPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/network-elements/\" click DNAC_DLL_EQPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/equipments/\" click DNAC_DLL_APDL \"/pages/tadeckar/assets-docs/ingestion/loaders/alerts/\" click DNAC_DLL_BMPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/bulletins/\" click DNAC_DLL_CPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/contracts/\" click DNAC_DLL_CLIPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/cli/\" click DNAC_DLL_CONFIGPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/config/\" click ELSE_BMPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/bulletins/\" click ELSE_NEPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/network-elements/\" click ELSE_EQPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/equipments/\" click ELSE_APDL \"/pages/tadeckar/assets-docs/ingestion/loaders/alerts/\" click ELSE_CPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/contracts/\" LIC -- LIC_DLM(DataLoadManager) LIC_DLM -- LPDL(LicenseParquetDataLoader) click LIC \"/pages/tadeckar/assets-docs/ingestion/managers/license-data-load-manager/\" click LIC_DLM \"/pages/tadeckar/assets-docs/ingestion/managers/data-load-manager/\" click LPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/licenses/\" HYB -- HYB_DLM(DataLoadManager) HYB_DLM -- HSPDL(HybridSubscriptionParquetDataLoader) click HYB \"/pages/tadeckar/assets-docs/ingestion/managers/hybrid-subscription-data-load-manager/\" click HYB_DLM \"/pages/tadeckar/assets-docs/ingestion/managers/data-load-manager/\" click HSPDL \"/pages/tadeckar/assets-docs/ingestion/loaders/hybrid-subscriptions/\"  Contents  Loaders   Managers   Schema   Types   Workflow   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/inv_upload_notification_prc/",
	"title": "inv_upload_notification_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 20 seconds.\n What does it do? Checks for rows in asset_inventory_notification that are in SUBMITTED state. Changes the state to INPROGRESS and calls the update_raw_data_process_prc stored procedure on the rows. On completion, changes the state to SUCCESS.   More Details    Gets a cursor on asset_inventory_notification where  processingStatus is SUBMITTED and no other rows of the selected customerId have processingStatus that is INPROGRESS   Get a count of the number of SUBMITTED rows and a count of the number of INPROGRESS rows. If SUBMITTED count is \u0026gt; 0 and INPROGRESS count is \u0026lt; 5, begin a read loop over the cursor that:  Check for any INPROGRESS rows on the given customerId. If the count is \u0026gt; 0, do not proceed. Select one SUBMITTED row from asset_inventory_notification that matches the customerId. Update processingStatus to be INPROGRESS for all rows that match customerId, wfId, processingStatus, mgmtSystemType, and mgmtSystemId of previously selected row. If \u0026gt; 0 rows were updated, call the update_raw_data_process_prc stored procedure with fields from the row. Update processingStatus to be SUCCESS for the affected rows.     \nReferenced Tables  asset_inventory_notification data_merge_logs  Referenced Stored Procedures  log_msg_prc update_raw_data_process_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/inventory-data-load-manager/",
	"title": "InventoryDataLoadManager",
	"tags": [],
	"description": "",
	"content": " View Source  The InventoryDataLoadManager is not currently in use.\n Loads data of the IBES_INV_UPLOAD File Type.\nFirstly, the IDLM tries to create a partition using the add_base_table_partions_prc stored procedure. If the partition already exists, processing is skipped.\nNext, a thread is spun up to load the data, executing the DataLoadManager, which in turn executes a Loader of IBES Data Load Type.\nThe NotificationManager is also used record the data load processing start/end times.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/iso_data_process_wrap_prc/",
	"title": "iso_data_process_wrap_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Call stored procedures to create summary views for iso data.   More Details    Get a count of the rows in the customer_wfid_info table that match customerId/wfId where  module is CSDF_AMP_TELE and wfIdStatus is I   If the count is 0, then insert a row. Call stored procedures  Call the iso_sum_table_deletion_prc stored procedure Call the iso_daily_trend_prc stored procedure Call the iso_weekly_and_monthly_trend_prc stored procedure Call the iso_feature_usage_prc stored procedure Call the iso_feature_insight_trend_prc stored procedure Call the iso_summary_view_prc stored procedure   Delete any row from the customer_wfid_info table matching customerId/wfId where wfIdStatus is A. Update wfIdStatus to A for any row from the customer_wfid_info table matching customerId/wfId where wfIdStatus is I. Delete any rows from the following tables that match customerId but don\u0026rsquo;t match wfId:  iso_daily_consumption_trend iso_daily_login_trend iso_feature_insight_trend iso_feature_usage iso_monthly_consumption_trend iso_monthly_login_trend iso_quarterly_consumption_trend iso_quarterly_login_trend iso_summary_view iso_weekly_consumption_trend iso_weekly_login_trend     \nReferenced Tables  amp_data_merge_logs customer_wfid_info iso_daily_consumption_trend iso_daily_login_trend iso_feature_insight_trend iso_feature_usage iso_monthly_consumption_trend iso_monthly_login_trend iso_quarterly_consumption_trend iso_quarterly_login_trend iso_summary_view iso_weekly_consumption_trend iso_weekly_login_trend  Referenced Stored Procedures  amp_log_msg_prc iso_daily_trend_prc iso_feature_insight_trend_prc iso_feature_usage_prc iso_sum_table_deletion_prc iso_summary_view_prc iso_weekly_and_monthly_trend_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/license-data-load-manager/",
	"title": "LicenseDataLoadManager",
	"tags": [],
	"description": "",
	"content": " View Source  Loads data of the MERAKI_LICENSE File Type.\nFirstly, the LDLM checks whether or not data for the given wfId has already been loaded. If the data has already been loaded, the data is cleared from the meraki_license_sum_view_stage staging table for the given customerId/wfId before execution proceeds to the next step.\nNext, a thread is spun up to load the data, executing the DataLoadManager, which in turn executes a Loader for the License Data Load Type.\nThe NotificationManager is also used record the data load processing start/end times, as well as to delete rows when wfId data has already been loaded.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/licenses/",
	"title": "Licenses",
	"tags": [],
	"description": "",
	"content": "Licenses are loaded by the LicenseParquetDataLoader.py.\nSchema Staging Tables  meraki_license_sum_view_stage  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/licenses/",
	"title": "Licenses Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class License { +Boolean isPDL +Long generatedAt +Long licenseEndDate +Long purchasedQuantity +String cavBUId +String cavBUName +String cavId +String cavName +String customerId +String licenseLevel +String licenseStatus +String licenseType +String orgId +String orgName +String sourceLicenseStatus +String wfId }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/",
	"title": "Loaders",
	"tags": [],
	"description": "",
	"content": "The IBES Data Loader State of the Glue workflow contains tasks for loading various datasets:\n Alerts   Bulletins   CLI   Config   Contracts   Equipments   Hybrid Subscriptions   Licenses   Network Elements   Subscriptions   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/log_msg_prc/",
	"title": "log_msg_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Inserts a row into the data_merge_logs table given customerId, wfId, function_name, and msg, along with the current timestamp.\nReferenced Tables  data_merge_logs  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/",
	"title": "Managers",
	"tags": [],
	"description": "",
	"content": "The main Glue function determines what to load using the file_type argument (see File Types). This argument is passed to a \u0026ldquo;Manager\u0026rdquo; to determine how the data is handled.\nThe DataFileManager is used in all scenarios. This contains logic to return a \u0026ldquo;Request Handler\u0026rdquo;, which is another \u0026ldquo;Manager\u0026rdquo; for a particular data type.\nThe \u0026ldquo;Request Handler\u0026rdquo; returned by the DataFileManager will be one of the following:\n InventoryDataLoadManager ParquetDataLoadManager LicenseDataLoadManager HybridSubscriptionDataLoadManager  Each of these implements a common interface:\nhandleRequest: Generic handler for loading data.\nsubmitDataLoadRequest: Begins the data loading job.\nprocessDataFile: Creates a thread pool to handle data loading.\nThe general flow of Manager execution is:\n handleRequest start.*Processing processDataFile submitDataLoadRequest  The submitDataLoadRequest function instantiates a DataLoadManager which returns a Loader class to further handle the request.\nMore Info  DataFileManager   DataLoadManager   HybridSubscriptionDataLoadManager   InventoryDataLoadManager   LicenseDataLoadManager   NotificationManager   ParquetInvDataLoadManager   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/meraki_lic_process_wrap_prc/",
	"title": "meraki_lic_process_wrap_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Creates summary views for meraki licenses.   More Details    Get a count of rows matching customerId/wfId in the customer_wfid_info table. If count is 0, add a row. Call the meraki_license_sum_view_prc stored procedure with customerId/wfId as input. Delete the row (if it exists) from the customer_wfid_info table where  customerId matches and module is MERAKI_LICENSE and wfIdStatus is A   Update wfIdStatus to A from the row in the customer_wfid_info table where  customerId matches and module is MERAKI_LICENSE and wfIdStatus is I   Delete the previous wfId rows from the meraki_license_sum_view_stage table where  customerId matches input and wfId does not match input and wfId does not match any wfId in asset_data_load_notification where processingStatus is SUBMITTED     \nReferenced Tables  amp_data_merge_logs asset_data_load_notification customer_wfid_info meraki_license_sum_view_stage  Referenced Stored Procedures  amp_log_msg_prc meraki_license_sum_view_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/meraki_license_notification_prc/",
	"title": "meraki_license_notification_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 30 seconds.\n What does it do? Checks for SUBMITTED rows in asset_data_load_notification. Transitions them to INPROGRESS state and then calls the meraki_lic_process_wrap_prc stored procedure. Afterward, updates the state to SUCCESS for the affected rows.   More Details    Gets a cursor over asset_data_load_notification where:  processingStatus is SUBMITTED and dataSource is MERAKI_LICENSE and the customerId has no other rows where processingStatus is INPROGRESS   Get counts for:  Number of rows in SUBMITTED state Number of rows in INPROGRESS state   Get the max number of parallel uploads from the upd_or_async_prop_master table If SUBMITTED count \u0026gt; 0 and INPROGRESS count \u0026lt; max parallel uploads, open a read loop that:  Checks if any rows are INPROGRESS for the current customerId. If so: does nothing, otherwise Gets the wfId Updates processingStatus to INPROGRESS on rows where customerId/wfId match and processingStatus is SUBMITTED. If any were updated, call the meraki_lic_process_wrap_prc stored procedure on the customerId/wfId. Update processingStatus to SUCCESS for affected rows.     \nReferenced Tables  amp_data_merge_logs asset_data_load_notification upd_or_async_prop_master  Referenced Stored Procedures  amp_log_msg_prc meraki_lic_process_wrap_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/network-elements/",
	"title": "Network Elements",
	"tags": [],
	"description": "",
	"content": "Network Elements are loaded by the NEParquetDataLoader.py.\nSchema Staging Tables  networkelement_dcc networkelement_dcn networkelement_ib_data networkelement_meraki networkelement_sub_stg networkelement_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/network-elements/",
	"title": "Network Elements Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class NetworkElement { +Boolean isCollector +Boolean isManagedNe +Boolean isPDL +Boolean isScanCapable +Integer nodeId +Long configUpdateCollTime +Long createdAt +Long generatedAt +Long installedMemory +Long lastReceivedDate +Long lastUpdateDate +Long ldos +Long licenseActivationDate +Long licenseEndDate +Long systemUpTime +Long timeOfLastReset +Long updatedAt +String CavBUName +String CavName +String OrganizationUnit_id +String S3ObjectDetails +String associatedWlcIp +String baseProductId +String cavbuid +String cavid +String collectorId +String createdBy +String customerId +String cxLevel +String datasource +String family +String genPartyId +String hostname +String ibAvailabilityFlag +String id +String imageName +String instanceStatus +String ipAddress +String itemTypeCode +String lastResetReason +String licenseLevel +String licenseStatus +String licenseType +String managedNeId +String managementAddress +String mgmtSystemAddr +String mgmtSystemHostname +String mgmtSystemId +String mgmtSystemName +String mgmtSystemTimeZone +String mgmtSystemType +String neId +String neName +String neRegistrationStatus +String networkId +String networkName +String networkUrl +String orgId +String orgName +String orgUrl +String partnerBeGeoId +String partnerBeGeoName +String partnerId +String partnerInfo +String partnerName +String prdtSetupClassificationCd +String productDescription +String productEndCrPartyIdInt +String productFamily +String productId +String productName +String productType +String reachabilityFailureReason +String reachabilityStatus +String registeredDeviceMoId +String role +String serialNumber +String serviceLevel +String siteHierarchy +String siteId +String siteNameHierarchy +String smartLicenseProductId +String smartLicenseVirtualAccountName +String softwareType +String softwareVersion +String solution +String solutionInfo +String sourceLicenseStatus +String sourceNeId +String subPartyId +String sysObjectId +String tagId +String tagName +String tags +String telemetryAvailabilityFlag +String termsAndContentCd +String udiProductIdentifier +String updatedBy +String usecase +String validatedImageName +String warrantyType +String wfId }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/notification-manager/",
	"title": "NotificationManager",
	"tags": [],
	"description": "",
	"content": " View Source  The NotificationManager is a utility class that houses a variety of functions for modifying the asset_inventory_notification and asset_data_load_notification tables.\nThe NotificationManager also houses the (seemingly unrelated) function to delete all rows matching a given customerId/wfId for a given table.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/managers/parquet-inv-data-load-manager/",
	"title": "ParquetInvDataLoadManager",
	"tags": [],
	"description": "",
	"content": " View Source  Loads data of the CIBES_INV_UPLOAD and DNAC_UPLOAD File Types.\nFirstly, the PIDLM checks the SQS message for mgmtSystemType. If the value of this property is not DCC_SUB, PIDLM tries to create a partition using the add_base_table_partions_prc stored procedure. Otherwise, processing begins regardless.\nWhen mgmtSystemType === \u0026quot;DCC_SUB\u0026quot;, and the partition is created successfully, processing begins. If the partition already exists, the base table partitions are cleared using the delete_raw_data_wfid_prc stored procedure, then processing begins.\nNext, a thread is spun up to load the data, executing the DataLoadManager, which in turn executes a Loader of either XAAS, DNAC, or CIBES Data Load Types, dependent on the mgmtSystemType. See mapping below.\nWhen mgmtSystemType is \u0026quot;DCC_SUB\u0026quot;, Data Load Type is XAAS.\nWhen mgmtSystemType is \u0026quot;DNAC_DL\u0026quot;, Data Load Type is DNAC.\nWhen mgmtSystemType is neither \u0026quot;DCC_SUB\u0026quot; nor \u0026quot;DNAC_DL\u0026quot;, Data Load Type is CIBES.\nThe NotificationManager is also used record the data load processing start/end times.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/recurring-events/",
	"title": "Recurring Events",
	"tags": [],
	"description": "",
	"content": "Event to Stored Procedure Mapping  athena_sub_process_01: athena_asset_notification_prc - every 10 seconds clean_base_part_invalid_status: clean_invalid_status_prc - every 1 day clean_stale_datamerge: clean_stale_datamerge_prc - every 30 minutes clean_stale_partitions: drop_stale_partions_prc - every 1 day clean_sum_part_invalid_status: clean_invalid_wfId_status_sum_tables_prc - every 1 day data_merge_process_01: inv_upload_notification_prc - every 20 seconds data_process_writer_cleanup_script: stale_uploads_cleanup_prc - every 1 day drop_pending_partitions: drop_pending_wfid_status_sum_tables_prc - every 1 day iso_data_merge_cleanup_process: cleanup_longrunning_iso_data_prc - every 4 hours iso_data_merge_process_01: amp_upload_notification_prc - every 30 seconds meraki_license_process_01: meraki_license_notification_prc - every 30 seconds retry_failed_data_process_writer: retry_failed_uploads_prc - every 12 hours stale_inprogress_upload_cleanup: stale_inprogress_data_cleanup_prc - every 30 minutes sub_data_merge_process_01: amp_subscription_notification_prc - every 30 seconds xass_asset_process_01: xaas_asset_notification_prc - every 30 seconds  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/retry_failed_uploads_prc/",
	"title": "retry_failed_uploads_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 12 hours.\n What does it do? Check for FAILED/SUSPENDED rows in asset_inventory_notification that were started over 12 hours ago and have not been retried. For each row, partitions are dropped if they exist and rows are deleted from the base_table_partition_info table. The row status is updated to RECEIVED. Also, all rows of the asset_contract_notification where the processing was started over 24 hours ago in FAILED/SUSPENDED state.   More Details    Gets a cursor over asset_inventory_notification where  processingStart is over 12 hours ago, also not null/undefined, and processingStatus is either FAILED or SUSPENDED and retrycount is 0   Open a read loop on the cursor that:  Checks if the wfId is already present in the customer_partition_info and if not  Check if any uploads for the same mgmtSystemId with processingStart in the last 12 hours are in SUCCESS state. If none are, then Call the drop_partions_prc on the customerId/wfId Delete the rows from the base_table_partition_info table Update the rows in asset_inventory_notification that are in FAILED/SUSPENDED state by:  setting the processingStatus to RECEIVED and setting the retryCount to 1 setting the recordType to either\na. INVENTORY_DATA_RECEIVED if mgmtSystemType is DNAC or CSDFIB, or\nb. CIBES_INVENTORY_DATA_RECEIVED otherwise       Update all rows in asset_contract_notification that were started over 24 hours ago and in FAILED/SUSPENDED state by:  setting processingStatus to RECEIEVED setting retryCount to 1     \nReferenced Tables  asset_contract_notification asset_inventory_notification base_table_partition_info customer_partition_info partition_automation_logs  Referenced Stored Procedures  drop_partions_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/",
	"title": "Schema",
	"tags": [],
	"description": "",
	"content": " View Source   Alerts Schema   Bulletins Schema   CLI Schema   Config Schema   Contracts Schema   Equipments Schema   Hybrid Subscriptions Schema   Licenses Schema   Network Elements Schema   Subscriptions Schema   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/services/",
	"title": "Services",
	"tags": [],
	"description": "",
	"content": "  cp-asset-group-api   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/staging-tables/",
	"title": "Staging Tables",
	"tags": [],
	"description": "",
	"content": "Alerts  alert_fn_dcc alert_fn_dcn alert_fn_ib_data alert_fn_meraki alert_fn_telemetry alert_fn_telemetry alert_hweox_dcc alert_hweox_dcn alert_hweox_ib_data alert_hweox_meraki alert_hweox_telemetry alert_hweox_telemetry alert_psirt_dcc alert_psirt_dcn alert_psirt_ib_data alert_psirt_meraki alert_psirt_telemetry alert_sweox_dcc alert_sweox_dcn alert_sweox_ib_data alert_sweox_meraki alert_sweox_telemetry alert_sweox_telemetry  Bulletins  alert_pas_hw_eox_bulletin_dcc alert_pas_hw_eox_bulletin_dcn alert_pas_hw_eox_bulletin_ib_data alert_pas_hw_eox_bulletin_ib_data alert_pas_hw_eox_bulletin_meraki alert_pas_hw_eox_bulletin_telemetry alert_pas_sw_eox_bulletin_dcc alert_pas_sw_eox_bulletin_dcn alert_pas_sw_eox_bulletin_ib_data alert_pas_sw_eox_bulletin_meraki alert_pas_sw_eox_bulletin_telemetry fn_bulletin_master_dcc fn_bulletin_master_dcn fn_bulletin_master_ib_data fn_bulletin_master_meraki fn_bulletin_master_telemetry sa_bulletin_master_dcc sa_bulletin_master_dcn sa_bulletin_master_ib_data sa_bulletin_master_meraki sa_bulletin_master_telemetry  CLI  cli_telemetry  Config  config_telemetry  Contracts  contractcoverage contractcoverage_dcc contractcoverage_dcn contractcoverage_ib_data contractcoverage_meraki contractcoverage_telemetry  Equipments  equipment_dcc equipment_dcn equipment_ib_data equipment_meraki equipment_telemetry equipment_telemetry  Hybrid Subscriptions  athena_subscription_stg  Network Elements  networkelement_dcc networkelement_dcn networkelement_ib_data networkelement_meraki networkelement_sub_stg networkelement_telemetry  Subscriptions  subscription_stg  Other  asset_feature_dcc asset_feature_dcn asset_feature_meraki asset_feature_telemetry  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/stale_inprogress_data_cleanup_prc/",
	"title": "stale_inprogress_data_cleanup_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 30 minutes.\n What does it do? Updates the processingStatus from INPROGRESS to SUCCESS for rows in the asset_inventory_notification table that started over 1 hour ago when partitionStatus in the customer_partition_info table is A (Active.)\nAlso updates the processingStatus from INPROGRESS to SUCCESS for rows in the asset_inventory_notification table that were started over 6 hours ago and are not found in customer_partition_info or base_table_partition_info.   More Details    Get a cursor over asset_inventory_notification where  processingStart is over 1 hour ago, also not null/undefined, and processingStatus is INPROGRESS partitionStatus from customer_partition_info table is A (Active)   Open a read loop on the cursor that updates the processingStatus to SUCCESS and sets processingEnd to the current time. Set processingStatus to SUCCESS for all rows of the asset_inventory_notification table where  processingStatus is INPROGRESS and processingStart is over 6 hours ago, also not null/undefined, and wfId is not found in customer_partition_info and wfId is not found in base_table_partition_info with partitionStatus as I and processingEnd is 0     \nReferenced Tables  asset_inventory_notification base_table_partition_info customer_partition_info data_merge_logs  Referenced Stored Procedures  log_msg_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/stale_uploads_cleanup_prc/",
	"title": "stale_uploads_cleanup_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs once every day.\n What does it do? Updates the processingStatus of 2-day-old rows in asset_inventory_notification to be FAILED after first verifying whether or not the device count for the customerId/wfId is changing within a 3-minute timespan.   More Details    Get a cursor over asset_inventory_notification where  processingStart is greater than 2 days ago, is also not null/undefined, and processingStatus is STARTED   Open a read loop on the cursor that  Gets a device count by  When mgmtSystemType is DNAC:  Get a count of rows in networkelement_telemetry table that match the row\u0026rsquo;s customerId/wfId.   Otherwise  Get a count of rows in networkelement_ib_data table that match the row\u0026rsquo;s customerId/wfId.     Sleeps for 180 seconds. Gets the device count again. If the 2 counts are the same (no devices have been added in 3 minutes,) update the processingStatus of the rows to FAILED.     \nReferenced Tables  asset_inventory_notification data_merge_logs networkelement_ib_data networkelement_telemetry  Referenced Stored Procedures  log_msg_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/",
	"title": "Stored Procedures",
	"tags": [],
	"description": "",
	"content": "  add_base_table_partions_prc   amp_log_msg_prc   amp_subscription_notification_prc   amp_upload_notification_prc   athena_asset_notification_prc   athena_dcc_dcn_data_process_prc   clean_invalid_status_prc   clean_invalid_wfId_status_sum_tables_prc   clean_stale_datamerge_prc   cleanup_longrunning_iso_data_prc   delete_raw_data_wfid_prc   drop_partions_prc   drop_pending_wfid_status_sum_tables_prc   drop_stale_partions_prc   inv_upload_notification_prc   iso_data_process_wrap_prc   log_msg_prc   meraki_lic_process_wrap_prc   meraki_license_notification_prc   retry_failed_uploads_prc   stale_inprogress_data_cleanup_prc   stale_uploads_cleanup_prc   sub_data_process_wrap_prc   update_raw_data_process_prc   xaas_asset_notification_prc   xaas_asset_process_wrap_prc   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/sub_data_process_wrap_prc/",
	"title": "sub_data_process_wrap_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Call stored procedures to create summary views for subscriptions, licenses, and contracts.   More Details    Get a count of the rows in the customer_wfid_info table that match customerId/wfId where  module is CSDF_SUBSCRIPTION and wfIdStatus is I   If the count is 0, then insert a row. Call stored procedures  Call the iso_subscription_prc stored procedure Call the iso_contract_sum_prc stored procedure Call the license_sum_view_prc stored procedure   Delete any row from the customer_wfid_info table matching customerId/wfId where wfIdStatus is A. Update wfIdStatus to A for any row from the customer_wfid_info table matching customerId/wfId where wfIdStatus is I. Delete any rows from the following tables that match customerId but don\u0026rsquo;t match wfId:  aav_subscriptions iso_contract_view license_sum_view     \nReferenced Tables  aav_subscriptions amp_data_merge_logs customer_wfid_info iso_contract_view license_sum_view  Referenced Stored Procedures  amp_log_msg_prc iso_contract_sum_prc iso_subscription_prc license_sum_view_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/loaders/subscriptions/",
	"title": "Subscriptions",
	"tags": [],
	"description": "",
	"content": "Subscriptions are loaded by the SubscriptionParquetDataLoader.py.\nSchema Staging Tables  subscription_stg  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/schema/subscriptions/",
	"title": "Subscriptions Schema",
	"tags": [],
	"description": "",
	"content": " classDiagram class Subscription{ +Integer erpSalesOrderNumber +Integer licenseTermInMonths +Long subscriptionCreateDate +Long subscriptionProductQuantity +Long termEndDate +Long termStartDate +String contractEntitlementDescr +String contractNumber +String coverageStatus +String endCustomerGuAddressLine1 +String endCustomerGuAddressLine2 +String endCustomerGuAddressLine3 +String endCustomerGuAddressLine4 +String endCustomerGuCityName +String endCustomerGuCountry +String endCustomerGuPostalCd +String endCustomerGuState +String managedNeId +String monetizationTypeCd +String neId +String partnerBeGeoId +String partnerBeGeoName +String partnerBeId +String partnerBeName +String serviceLevel +String serviceLevelDescription +String serviceProgram +String subscriptionBillToSiteUseCustName +String subscriptionBillToSiteUseId +String subscriptionProductClass +String subscriptionReferenceId +String subscriptionStatus +String subscriptionType +String termsAndContentCd +String webOrderId }  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/types/",
	"title": "Types",
	"tags": [],
	"description": "",
	"content": "  Data Load Types   File Types   "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/update_raw_data_process_prc/",
	"title": "update_raw_data_process_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Gets counts for devices by mgmtSystemType. If counts are 0, removes relevant partitions. Otherwise, call stored procedures to create summary views and merge data.   More Details    Get a count of rows matching customerId/wfId from tables based on mgmtSystemType stored procedure input parameter:    mgmtSystemType Table     CSDFIB networkelement_ib_data   DNAC networkelement_telemetry   APIC networkelement_dcn   MERAKI networkelement_meraki   DCC networkelement_dcc     If the row count is 0:  Update the partitionStatus to D for the customerId/wfId row in the base_table_partition_info table. Call the drop_dnac_old_partions_prc stored procedure Update the processingStatus to NODATA for rows in the asset_inventory_notification table where customerId/wfId match. Exit the current procedure.   Otherwise, count the number of rows in the base_table_partition_info table where the row matches the stored procedure\u0026rsquo;s input parameters. If the count is \u0026gt; 0:  Set the partitionStatus to D (Deleted) for the matching row in the base_table_partition_info table for any rows that were in A (Active) state. Set the partitionStatus to A (Active) for the matching row in the base_table_partition_info table for any rows that were in I (Inprogress) state. Call the drop_dnac_old_partions_prc stored procedure Delete rows in the base_table_partition_info table that are marked with partitionStatus as D.   Call the following stored procedures:  add_sumview_table_partions_prc data_merge_prc sum_table_data_update_prc update_sum_data_process_prc   Update the processingStatus to SUCCESS for relevant rows from the asset_inventory_notification table.   \nReferenced Tables  asset_inventory_notification base_table_partition_info data_merge_logs networkelement_dcc networkelement_dcn networkelement_ib_data networkelement_meraki networkelement_telemetry  Referenced Stored Procedures  add_sumview_table_partions_prc data_merge_prc drop_dnac_old_partions_prc log_msg_prc sum_table_data_update_prc update_sum_data_process_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/ingestion/workflow/",
	"title": "Workflow",
	"tags": [],
	"description": "",
	"content": " graph LR; subgraph Verify Data Load VDL{Verify Data load} end subgraph Notify Success VDL -- NS(Notify) end subgraph Notify Failure VDL -- NF(Notify) end subgraph IBES Data Loader LBM(Load Bulletin Master) -- VDL LNE(Load Network Element) -- VDL LE(Load Equipments) -- VDL LA(Load Alerts) -- VDL LC(Load Contracts) -- VDL LF(Load Features) -- VDL end subgraph Pre Processor P(Preprocess) -- LBM P(Preprocess) -- LNE P(Preprocess) -- LE P(Preprocess) -- LA P(Preprocess) -- LC P(Preprocess) -- LF end  The Glue workflow begins at the Pre Processor state. This state is type: Pass which allows for state that does not produce any work. Next, the IBES Data Loader state loads the data from S3 into the SQL DB. Finally, the Verify Data Load state checks that all tasks within the IBES Data Loader state were successful. Based on the results, state transitions to either Notify Success or Notify Failure, at which time the workflow is complete.\nSee Loaders for more information on the tasks within the IBES Data Loader state.\n"
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/xaas_asset_notification_prc/",
	"title": "xaas_asset_notification_prc",
	"tags": [],
	"description": "",
	"content": " This procedure runs every 30 seconds.\n What does it do? Checks for rows in asset_data_load_notification in SUBMITTED state where dataSource is DCC_SUB. Transitions the rows to INPROGRESS state and calls the xaas_asset_process_wrap_prc stored procedure. Afterwards, updates the state of the rows to SUCCESS.   More Details    Get a cursor over asset_data_load_notification where  processingStatus is SUBMITTED and dataSource is DCC_SUB None of the rows for same customerId are INPROGRESS   Get counts for  rows in SUBMITTED state rows in INPROGRESS state   Get the max parallel limit of amp uploads from the upd_or_async_prop_master table. If the SUBMITTED count is \u0026gt; 0 and INPROGRESS count is less than the max parallel limit, open a read loop that:  Counts the number of INPROGRESS rows for the given customerId If the count is 0 (none INPROGRESS), get the wfId from a SUBMITTED row. Update the status to INPROGRESS of SUBMITTED rows with the wfId from the previous step. Call the xaas_asset_process_wrap_prc stored procedure with input data from the row. Update the status to SUCCESS on the affected rows.     \nReferenced Tables  amp_data_merge_logs asset_data_load_notification upd_or_async_prop_master  Referenced Stored Procedures  amp_log_msg_prc xaas_asset_process_wrap_prc  "
},
{
	"uri": "https://www-github3.cisco.com/pages/tadeckar/assets-docs/database/stored-procedures/xaas_asset_process_wrap_prc/",
	"title": "xaas_asset_process_wrap_prc",
	"tags": [],
	"description": "",
	"content": "What does it do? Call stored procedures to create summary views for XAAS assets.   More Details    Get a count of the rows in the customer_wfid_info table that match customerId/wfId where  module is DCC_SUB and wfIdStatus is I   If the count is 0, then insert a row. Call stored procedures  Call the xaas_sub_data_process_prc stored procedure Call the xaas_sub_license_prc stored procedure   Delete any row from the customer_wfid_info table matching customerId/wfId where wfIdStatus is A. Update wfIdStatus to A for any row from the customer_wfid_info table matching customerId/wfId where wfIdStatus is I. Delete any rows from the following tables that match customerId but don\u0026rsquo;t match wfId:  networkelement_sub_stg subscription_stg     \nReferenced Tables  amp_data_merge_logs customer_wfid_info networkelement_sub_stg subscription_stg  Referenced Stored Procedures  amp_log_msg_prc log_msg_prc xaas_sub_data_process_prc xaas_sub_license_prc  "
}]